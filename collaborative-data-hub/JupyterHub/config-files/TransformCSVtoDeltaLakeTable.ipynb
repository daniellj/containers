{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40353901",
   "metadata": {},
   "source": [
    "### O.S. process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b29426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hostname: jupyter-hub\n",
      "- JAVA_HOME:/opt/jdk\n",
      "- PYSPARK_SUBMIT_ARGS:--packages io.delta:delta-core_2.12:2.1.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" pyspark-shell\n",
      "- PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/jdk:/opt/jdk/bin\n",
      "dbus-python==1.2.18\n",
      "delta-spark==2.1.0\n",
      "gyp==0.1\n",
      "importlib-metadata==5.0.0\n",
      "numpy==1.23.4\n",
      "py4j==0.10.9.5\n",
      "pyarrow==9.0.0\n",
      "PyGObject==3.42.1\n",
      "pyspark==3.3.0\n",
      "zipp==3.9.0\n",
      "java version \"1.8.0_341\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_341-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.341-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from os import environ\n",
    "\n",
    "!host=$(hostname) \n",
    "!ip=$(ifconfig | grep 'inet ' | grep -v '127.0.0.1' | cut -c 7-17)\n",
    "!echo \"hostname: $(hostname)\"\n",
    "\n",
    "# create path if not exists\n",
    "#!mkdir -p ~/notebooks/data/\n",
    "\n",
    "# download if not exists\n",
    "#!wget -nc https://files.grouplens.org/datasets/movielens/ml-25m.zip -P ~/notebooks/data/\n",
    "\n",
    "# unzip if not exists\n",
    "#!unzip -n ~/notebooks/data/ml-25m.zip -d ~/notebooks/data/\n",
    "\n",
    "#!ls -las /home/admin/notebooks/data/ml-25m\n",
    "\n",
    "# check environment variables: JAVA_HOME\n",
    "!export JAVA_HOME=/opt/jdk\n",
    "environ[\"JAVA_HOME\"] = \"/opt/jdk\"\n",
    "!echo \"- JAVA_HOME:$JAVA_HOME\"\n",
    "\n",
    "# check environment variables: PYSPARK_SUBMIT_ARGS\n",
    "!export PYSPARK_SUBMIT_ARGS='--packages io.delta:delta-core_2.12:2.1.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" pyspark-shell'\n",
    "environ[\"PYSPARK_SUBMIT_ARGS\"]='--packages io.delta:delta-core_2.12:2.1.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" pyspark-shell'\n",
    "!echo \"- PYSPARK_SUBMIT_ARGS:$PYSPARK_SUBMIT_ARGS\"\n",
    "\n",
    "# check environment variables: PATH\n",
    "!export PATH=$PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/jdk:/opt/jdk/bin\n",
    "environ[\"PATH\"] = \"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/jdk:/opt/jdk/bin\"\n",
    "!echo \"- PATH:$PATH\"\n",
    "\n",
    "!pip freeze\n",
    "\n",
    "# check java version\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b77c33",
   "metadata": {},
   "source": [
    "### Function to reduce memory usage in Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1825f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%\\n'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d353c",
   "metadata": {},
   "source": [
    "### Connect from Apache Spark Cluster - without Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bb9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from os import environ\n",
    "#environ[\"SPARK_HOME\"] = '/opt/apache-spark'\n",
    "#environ[\"PATH\"] = '$PATH:/opt/jdk:/opt/jdk/bin:/opt/apache-spark:/opt/apache-spark/bin:/opt/apache-spark/sbin'\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrameReader\n",
    "\n",
    "#If you need to stop SparkContext (sc) or SparkSession\n",
    "if 's_session' in locals():\n",
    "    s_session.stop()\n",
    "if 's_context' in locals():\n",
    "    s_context.stop()\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.setAppName(\"app_data_lake\") \\\n",
    ".setMaster(\"spark://spark-master:7077\") \\\n",
    ".setSparkHome(\"/opt/apache-spark\")\n",
    "\n",
    "s_context = SparkContext(conf=conf).getOrCreate()\n",
    "s_session = SparkSession(sparkContext=s_context)\n",
    "#builder = s_session.builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb7ed0",
   "metadata": {},
   "source": [
    "### Read .CSV from SFTP and load into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaec2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysftp\n",
    "from pandas import read_csv as pandas_read_csv\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# SFTP config connection\n",
    "cnopts = pysftp.CnOpts()\n",
    "cnopts.hostkeys = None\n",
    "environ[\"FTP_HOST\"] = 'sftp-01' # sftp-01 = 172.19.0.15\n",
    "environ[\"FTP_PORT\"] = '2222'\n",
    "environ[\"FTP_USER\"] = 'admin'\n",
    "environ[\"FTP_PASS\"] = 'admin'\n",
    "\n",
    "# CSV schema\n",
    "schema_doc = {\n",
    "                \"tags.csv\": StructType([StructField(\"userId\", IntegerType(), True),\n",
    "                                     StructField(\"movieId\", IntegerType(), True),\n",
    "                                     StructField(\"tag\", StringType(), True),\n",
    "                                     StructField(\"timestamp\", IntegerType(), True)]),\n",
    "                \"ratings.csv\": StructType([StructField(\"userId\", IntegerType(), True),\n",
    "                                     StructField(\"movieId\", IntegerType(), True),\n",
    "                                     StructField(\"rating\", FloatType(), True),\n",
    "                                     StructField(\"timestamp\", IntegerType(), True)])\n",
    "                }\n",
    "\n",
    "chunksize=500000\n",
    "sftp_file=\"tags.csv\" # 1.093.000 lines\n",
    "#sftp_file=\"ratings.csv\" # 25.000.000 lines\n",
    "\n",
    "# open SFTP connection\n",
    "with pysftp.Connection(environ[\"FTP_HOST\"], port = int(environ[\"FTP_PORT\"]), username = environ[\"FTP_USER\"], password = environ[\"FTP_PASS\"], cnopts=cnopts) as connection:\n",
    "    print(\"Connection succesfully established…\\n\")\n",
    "    # open the file\n",
    "    with connection.open(remote_file = f\"/data/{sftp_file}\", mode='r') as file:\n",
    "\n",
    "        i = 1\n",
    "        for reader in pandas_read_csv(file, sep=',', chunksize=chunksize):\n",
    "            chnk = \"0 until \" + str(chunksize) if i==1 else str(((chunksize * i) - chunksize) + 1) + \" until \" + str(chunksize * i)\n",
    "            print(f\"Chunksize block = line {chnk}\")\n",
    "            reader=reduce_mem_usage(df=reader)\n",
    "            if i == 1:\n",
    "                data = s_session.createDataFrame(data=reader, schema=schema_doc.get(sftp_file, sftp_file.split('.')[0]))\n",
    "            else:\n",
    "                new_data = s_session.createDataFrame(data=reader, schema=schema_doc.get(sftp_file, sftp_file.split('.')[0]))\n",
    "                data = data.union(new_data)\n",
    "                del new_data\n",
    "            i = i + 1\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a7d82",
   "metadata": {},
   "source": [
    "### Resume of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68de79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( \"- sparkSession: \", data.sparkSession, '\\n' )\n",
    "print(\"- Object: \", type(data), \"\\n\")\n",
    "print( \"- schema: \", data.schema, '\\n' )\n",
    "print( \"- printSchema: \", data.printSchema(), '\\n' )\n",
    "print( \"- isStreaming: \", data.isStreaming, '\\n' )\n",
    "print( \"- columns: \", data.columns, '\\n' )\n",
    "print( \"- dtypes: \", data.dtypes, '\\n' )\n",
    "print( \"- head: \", data.head(10), '\\n' )\n",
    "print( \"- show: \", data.show(10), '\\n' )\n",
    "print( \"- isEmpty: \", data.isEmpty(), '\\n' )\n",
    "print(\"- cache\", data.cache(), '\\n' ) # Persists the DataFrame with the default storage level (MEMORY_AND_DISK)\n",
    "print( \"- persist: \", data.persist(), '\\n' ) # Sets the storage level to persist the contents of the DataFrame across operations after the first time it is computed.\n",
    "print( \"- storageLevel: \", data.storageLevel, '\\n' )\n",
    "print( \"- count: \", data.count(), '\\n' )\n",
    "if sftp_file==\"ratings.csv\":\n",
    "    print( \"- correlation between rating and timestamp: \", data.corr(\"rating\", \"timestamp\"), '\\n' )\n",
    "    print( \"- covariance between rating and timestamp: \", data.cov(\"rating\", \"timestamp\"), '\\n' ) # Calculate the sample covariance for the given columns, specified by their names, as a double value\n",
    "    print( \"- descriptive statistics: \", data.describe([\"userId\", \"movieId\", \"rating\", \"timestamp\"]).show(), '\\n' )\n",
    "#print( \"- summary: \", data.summary(), '\\n' ) # Computes specified statistics for numeric and string columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ec9ea",
   "metadata": {},
   "source": [
    "### Write DataFrame in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.csv(\"hdfs://hdpmaster:9000/users/hduser/teste1.csv\", header=True, mode=\"ignore\")\n",
    "data.write.parquet(\"hdfs://hdpmaster:9000/users/hduser/teste1.parquet\", mode=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16c46b",
   "metadata": {},
   "source": [
    "### Read data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ec5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load_csv = s_session.read.csv(\"hdfs://hdpmaster:9000/users/hduser/teste1.csv\", header='true', inferSchema='true')\n",
    "df_load_parquet = s_session.read.parquet(\"hdfs://hdpmaster:9000/users/hduser/teste1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CSV FILE:\", \"\\n\")\n",
    "print( \"- sparkSession: \", df_load_csv.sparkSession, '\\n' )\n",
    "print(\"- Object: \", type(df_load_csv), \"\\n\")\n",
    "print( \"- schema: \", df_load_csv.schema, '\\n' )\n",
    "print( \"- printSchema: \", df_load_csv.printSchema(), '\\n' )\n",
    "print( \"- isStreaming: \", df_load_csv.isStreaming, '\\n' )\n",
    "print( \"- columns: \", df_load_csv.columns, '\\n' )\n",
    "print( \"- dtypes: \", df_load_csv.dtypes, '\\n' )\n",
    "print( \"- head: \", df_load_csv.head(10), '\\n' )\n",
    "print( \"- show: \", df_load_csv.show(10), '\\n' )\n",
    "print(\"##########################################################\")\n",
    "print(\"PARQUET FILE:\", \"\\n\")\n",
    "print( \"- sparkSession: \", df_load_parquet.sparkSession, '\\n' )\n",
    "print(\"- Object: \", type(df_load_parquet), \"\\n\")\n",
    "print( \"- schema: \", df_load_parquet.schema, '\\n' )\n",
    "print( \"- printSchema: \", df_load_parquet.printSchema(), '\\n' )\n",
    "print( \"- isStreaming: \", df_load_parquet.isStreaming, '\\n' )\n",
    "print( \"- columns: \", df_load_parquet.columns, '\\n' )\n",
    "print( \"- dtypes: \", df_load_parquet.dtypes, '\\n' )\n",
    "print( \"- head: \", df_load_parquet.head(10), '\\n' )\n",
    "print( \"- show: \", df_load_parquet.show(10), '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a775b5",
   "metadata": {},
   "source": [
    "### Create a temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_session.sql(\"CREATE TEMPORARY VIEW teste USING parquet OPTIONS (path \\\"hdfs://hdpmaster:9000/users/hduser/teste1.parquet\\\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_session.sql(\"select * from teste limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc66831",
   "metadata": {},
   "source": [
    "### Connect from Apache Spark Cluster - with Delta Lake Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0e2fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/srv/jupyterhub/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/admin/.ivy2/cache\n",
      "The jars for the packages stored in: /home/admin/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a5918e50-88cc-423e-a711-0cef267a5d22;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 164ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a5918e50-88cc-423e-a711-0cef267a5d22\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/20 17:57:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/20 17:57:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#If you need to stop SparkContext (sc) or SparkSession\n",
    "if 's_session' in locals():\n",
    "    s_session.stop()\n",
    "if 's_context' in locals():\n",
    "    s_context.stop()\n",
    "\n",
    "#If you need to stop SparkContext (sc) or SparkSession\n",
    "if 's_session_dl' in locals():\n",
    "    s_session_dl.stop()\n",
    "if 's_context_dl' in locals():\n",
    "    s_context_dl.stop()\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.setAppName(\"app_delta_lake\") \\\n",
    ".setMaster(\"spark://spark-master:7077\")\n",
    "#.setSparkHome(\"/opt/apache-spark\")\n",
    "#.set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\")\n",
    "#.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "\n",
    "s_context_dl = SparkContext(conf=conf).getOrCreate()\n",
    "s_session_dl = SparkSession(sparkContext=s_context_dl)\n",
    "builder = s_session_dl.builder \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "\n",
    "#my_packages = [\"io.delta:delta-core_2.12:2.1.0\"]\n",
    "#s_session_delta = configure_spark_with_delta_pip(spark_session_builder=builder, extra_packages=my_packages).getOrCreate()\n",
    "\n",
    "s_session_delta = configure_spark_with_delta_pip(spark_session_builder=builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b75d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter-hub python version: 3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]\n",
      "context pyspark version: 3.3.0\n",
      "context java spark version: 3.3.0\n",
      "context hadoop version = 3.3.2\n"
     ]
    }
   ],
   "source": [
    "from sys import version as sys_version\n",
    "\n",
    "print('jupyter-hub python version:', sys_version)\n",
    "print('context pyspark version:', s_context_dl.version)\n",
    "print('context java spark version:', s_context_dl._jsc.version())\n",
    "print(f'context hadoop version = {s_context_dl._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddd620",
   "metadata": {},
   "source": [
    "### Read .CSV from SFTP and load into a Pandas DataFrame - with Delta Lake Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1773076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection succesfully established…\n",
      "\n",
      "Chunksize block = line 0 until 500000\n",
      "Memory usage of dataframe is 15.26 MB\n",
      "Memory usage after optimization is: 8.99 MB\n",
      "Decreased by 41.1%\n",
      "\n",
      "Chunksize block = line 500001 until 1000000\n",
      "Memory usage of dataframe is 15.26 MB\n",
      "Memory usage after optimization is: 8.97 MB\n",
      "Decreased by 41.2%\n",
      "\n",
      "Chunksize block = line 1000001 until 1500000\n",
      "Memory usage of dataframe is 2.85 MB\n",
      "Memory usage after optimization is: 1.88 MB\n",
      "Decreased by 33.9%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pysftp\n",
    "from pandas import read_csv as pandas_read_csv\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import current_timestamp, date_format\n",
    "\n",
    "# SFTP config connection\n",
    "cnopts = pysftp.CnOpts()\n",
    "cnopts.hostkeys = None\n",
    "environ[\"FTP_HOST\"] = 'sftp-01' # sftp-01 = 172.19.0.15\n",
    "environ[\"FTP_PORT\"] = '2222'\n",
    "environ[\"FTP_USER\"] = 'admin'\n",
    "environ[\"FTP_PASS\"] = 'admin'\n",
    "\n",
    "# CSV schema\n",
    "schema_doc = {\n",
    "                \"tags.csv\": StructType([StructField(\"userId\", IntegerType(), True),\n",
    "                                     StructField(\"movieId\", IntegerType(), True),\n",
    "                                     StructField(\"tag\", StringType(), True),\n",
    "                                     StructField(\"timestamp\", IntegerType(), True)]),\n",
    "                \"ratings.csv\": StructType([StructField(\"userId\", IntegerType(), True),\n",
    "                                     StructField(\"movieId\", IntegerType(), True),\n",
    "                                     StructField(\"rating\", FloatType(), True),\n",
    "                                     StructField(\"timestamp\", IntegerType(), True)])\n",
    "                }\n",
    "\n",
    "chunksize=500000\n",
    "sftp_file=\"tags.csv\" # 1.093.000 lines\n",
    "#sftp_file=\"ratings.csv\" # 25.000.000 lines\n",
    "\n",
    "# open SFTP connection\n",
    "with pysftp.Connection(environ[\"FTP_HOST\"], port = int(environ[\"FTP_PORT\"]), username = environ[\"FTP_USER\"], password = environ[\"FTP_PASS\"], cnopts=cnopts) as connection:\n",
    "    print(\"Connection succesfully established…\\n\")\n",
    "    # open the file\n",
    "    with connection.open(remote_file = f\"/data/{sftp_file}\", mode='r') as file:\n",
    "\n",
    "        i = 1\n",
    "        for reader in pandas_read_csv(file, sep=',', chunksize=chunksize):\n",
    "            chnk = \"0 until \" + str(chunksize) if i==1 else str(((chunksize * i) - chunksize) + 1) + \" until \" + str(chunksize * i)\n",
    "            print(f\"Chunksize block = line {chnk}\")\n",
    "            reader=reduce_mem_usage(df=reader)\n",
    "            if i == 1:\n",
    "                data_dl = s_session_delta.createDataFrame(data=reader, schema=schema_doc.get(sftp_file, sftp_file.split('.')[0]))\n",
    "            else:\n",
    "                new_data_dl = s_session_delta.createDataFrame(data=reader, schema=schema_doc.get(sftp_file, sftp_file.split('.')[0]))\n",
    "                data_dl = data_dl.union(new_data_dl)\n",
    "                del new_data_dl\n",
    "            i = i + 1\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c688a92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_dl = data_dl.withColumn(\"created_datetime\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a417aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+--------------------+\n",
      "|userId|movieId|                 tag| timestamp|    created_datetime|\n",
      "+------+-------+--------------------+----------+--------------------+\n",
      "|     3|    260|             classic|1439472355|2022-10-20 17:40:...|\n",
      "|     3|    260|              sci-fi|1439472256|2022-10-20 17:40:...|\n",
      "|     4|   1732|         dark comedy|1573943598|2022-10-20 17:40:...|\n",
      "|     4|   1732|      great dialogue|1573943604|2022-10-20 17:40:...|\n",
      "|     4|   7569|    so bad it's good|1573943455|2022-10-20 17:40:...|\n",
      "|     4|  44665|unreliable narrators|1573943619|2022-10-20 17:40:...|\n",
      "|     4| 115569|               tense|1573943077|2022-10-20 17:40:...|\n",
      "|     4| 115713|artificial intell...|1573942979|2022-10-20 17:40:...|\n",
      "|     4| 115713|       philosophical|1573943033|2022-10-20 17:40:...|\n",
      "|     4| 115713|               tense|1573943042|2022-10-20 17:40:...|\n",
      "+------+-------+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_dl.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4925d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/20 16:16:45 WARN TaskSetManager: Stage 1 contains a task of very large size (1024 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=================================================>       (31 + 5) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- count:  1093360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print( \"- count: \", data_dl.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b2304",
   "metadata": {},
   "source": [
    "### Create tables WITHOUT the metastore - Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba494271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"CREATE SCHEMA IF NOT EXISTS bronze LOCATION 'hdfs://hdpmaster:9000/deltalake/bronze';\")\n",
    "s_session_delta.sql(\"CREATE SCHEMA IF NOT EXISTS silver LOCATION 'hdfs://hdpmaster:9000/deltalake/silver';\")\n",
    "s_session_delta.sql(\"CREATE SCHEMA IF NOT EXISTS gold LOCATION 'hdfs://hdpmaster:9000/deltalake/gold';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_session_delta.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bronze.tags (\n",
    "      userId INT,\n",
    "      movieId INT,\n",
    "      tag STRING,\n",
    "      timestamp INT\n",
    "    ) USING DELTA\n",
    "      LOCATION 'hdfs://hdpmaster:9000/deltalake/bronze/tags'\n",
    "      COMMENT 'Table to store movie tags.';\n",
    "    \"\"\")\n",
    "\n",
    "#data_dl.createOrReplaceTempView(\"bronze.tags\")\n",
    "\n",
    "s_session_delta.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bronze.ratings (\n",
    "      userId INT,\n",
    "      movieId INT,\n",
    "      rating FLOAT,\n",
    "      timestamp INT\n",
    "    ) USING DELTA\n",
    "      LOCATION 'hdfs://hdpmaster:9000/deltalake/bronze/ratings'\n",
    "      COMMENT 'Table to store movie ratings.';\n",
    "    \"\"\")\n",
    "\n",
    "#data_dl.createOrReplaceTempView(\"bronze.ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "901a3c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|  default|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_session_delta.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9a3ce49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|  default|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_session_delta.sql(\"SHOW SCHEMAS;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1ec544a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(info_name='Namespace Name', info_value='default'),\n",
       " Row(info_name='Comment', info_value='default database'),\n",
       " Row(info_name='Location', info_value='file:/home/admin/notebooks/spark-warehouse'),\n",
       " Row(info_name='Owner', info_value='admin'),\n",
       " Row(info_name='Properties', info_value='')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"\"\"DESCRIBE SCHEMA EXTENDED default\"\"\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06792e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(info_name='Namespace Name', info_value='bronze'),\n",
       " Row(info_name='Comment', info_value=''),\n",
       " Row(info_name='Location', info_value='file:/home/admin/notebooks/spark-warehouse/bronze.db'),\n",
       " Row(info_name='Owner', info_value='admin'),\n",
       " Row(info_name='Properties', info_value='')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"\"\"DESCRIBE SCHEMA EXTENDED bronze\"\"\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1cc8a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|     tags|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_session_delta.sql(\"SHOW TABLES IN default;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbce6a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|   bronze|     tags|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_session_delta.sql(\"SHOW TABLES IN bronze;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d990765c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(namespace='bronze', tableName='tags', isTemporary=False, information='Database: bronze\\nTable: tags\\nCreated Time: Thu Oct 20 17:59:11 GMT 2022\\nLast Access: UNKNOWN\\nCreated By: Spark 3.3.0\\nType: EXTERNAL\\nProvider: delta\\nComment: Table to store the genre for each movie.\\nLocation: hdfs://hdpmaster:9000/deltalake/bronze.tags\\nPartition Provider: Catalog\\n')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"SHOW TABLE EXTENDED IN bronze like 'tags'\").head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b7745",
   "metadata": {},
   "source": [
    "### Create tables WITH the metastore - Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5441ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"CREATE SCHEMA IF NOT EXISTS bronze LOCATION 'hdfs://hdpmaster:9000/deltalake/bronze';\")\n",
    "s_session_delta.sql(\"CREATE SCHEMA IF NOT EXISTS silver LOCATION 'hdfs://hdpmaster:9000/deltalake/silver';\")\n",
    "s_session_delta.sql(\"CREATE SCHEMA IF NOT EXISTS gold LOCATION 'hdfs://hdpmaster:9000/deltalake/gold';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5692f3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(info_name='Namespace Name', info_value='bronze'),\n",
       " Row(info_name='Comment', info_value=''),\n",
       " Row(info_name='Location', info_value='hdfs://hdpmaster:9000/deltalake/bronze'),\n",
       " Row(info_name='Owner', info_value='admin'),\n",
       " Row(info_name='Properties', info_value='')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"\"\"DESCRIBE SCHEMA EXTENDED bronze\"\"\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13f40c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(info_name='Namespace Name', info_value='silver'),\n",
       " Row(info_name='Comment', info_value=''),\n",
       " Row(info_name='Location', info_value='file:/home/admin/notebooks/spark-warehouse/silver.db'),\n",
       " Row(info_name='Owner', info_value='admin'),\n",
       " Row(info_name='Properties', info_value='')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"\"\"DESCRIBE SCHEMA EXTENDED silver\"\"\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ebdfff73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(info_name='Namespace Name', info_value='gold'),\n",
       " Row(info_name='Comment', info_value=''),\n",
       " Row(info_name='Location', info_value='hdfs://hdpmaster:9000/deltalake/gold'),\n",
       " Row(info_name='Owner', info_value='admin'),\n",
       " Row(info_name='Properties', info_value='')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"\"\"DESCRIBE SCHEMA EXTENDED gold\"\"\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae32dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/azure/databricks/delta/table-properties\n",
    "s_session_delta.conf.set(\"delta.autoOptimize.autoCompact\", \"true\")\n",
    "s_session_delta.conf.set(\"delta.autoOptimize.optimizeWrite\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd095acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7fbeec87c7f0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create \"bronze.tags\" table in the metastore\n",
    "DeltaTable.createIfNotExists(sparkSession=s_session_delta) \\\n",
    "  .tableName(\"bronze.tags\") \\\n",
    "  .addColumn(\"userId\", dataType=IntegerType(), nullable=True) \\\n",
    "  .addColumn(\"movieId\", dataType=IntegerType(), nullable=True) \\\n",
    "  .addColumn(\"tag\", dataType=StringType(), nullable=True, comment = \"Movie genre.\") \\\n",
    "  .addColumn(\"timestamp\", dataType=IntegerType(), nullable=True) \\\n",
    "  .addColumn(\"created_datetime\", dataType=TimestampType(), nullable=False) \\\n",
    "  .addColumn(\"created_date_year\", dataType=IntegerType(), nullable=False, generatedAlwaysAs=\"YEAR(created_datetime)\") \\\n",
    "  .addColumn(\"created_date_month\", dataType=IntegerType(), nullable=False, generatedAlwaysAs=\"MONTH(created_datetime)\") \\\n",
    "  .addColumn(\"created_date_day\", dataType=IntegerType(), nullable=False, generatedAlwaysAs=\"DAY(created_datetime)\") \\\n",
    "  .addColumn(\"modified_datetime\", dataType=TimestampType(), nullable=True) \\\n",
    "  .comment(\"Table to store the genre for each movie.\") \\\n",
    "  .property(\"description\", \"Table to store the genre for each movie.\") \\\n",
    "  .location(\"hdfs://hdpmaster:9000/deltalake/bronze/tags\") \\\n",
    "  .partitionedBy(\"created_date_year\", \"created_date_month\") \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb9f4684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7fbeec87e200>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create \"bronze.ratings\" table in the metastore\n",
    "DeltaTable.createIfNotExists(sparkSession=s_session_delta) \\\n",
    "  .tableName(\"bronze.ratings\") \\\n",
    "  .addColumn(\"userId\", dataType=IntegerType(), nullable=True) \\\n",
    "  .addColumn(\"movieId\", dataType=IntegerType(), nullable=True) \\\n",
    "  .addColumn(\"tag\", dataType=StringType(), nullable=True, comment = \"Movie genre.\") \\\n",
    "  .addColumn(\"timestamp\", dataType=IntegerType(), nullable=True) \\\n",
    "  .addColumn(\"created_datetime\", dataType=TimestampType(), nullable=False) \\\n",
    "  .addColumn(\"created_date_year\", dataType=IntegerType(), nullable=False, generatedAlwaysAs=\"YEAR(created_datetime)\") \\\n",
    "  .addColumn(\"created_date_month\", dataType=IntegerType(), nullable=False, generatedAlwaysAs=\"MONTH(created_datetime)\") \\\n",
    "  .addColumn(\"created_date_day\", dataType=IntegerType(), nullable=False, generatedAlwaysAs=\"DAY(created_datetime)\") \\\n",
    "  .addColumn(\"modified_datetime\", dataType=TimestampType(), nullable=True) \\\n",
    "  .comment(\"Movies classification ratings into a time series.\") \\\n",
    "  .property(\"description\", \"Movies classification ratings into a time series.\") \\\n",
    "  .location(\"hdfs://hdpmaster:9000/deltalake/bronze/ratings\") \\\n",
    "  .partitionedBy(\"created_date_year\", \"created_date_month\") \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71c9f1",
   "metadata": {},
   "source": [
    "### Write data in table - Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d24e8bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s_session_delta.sql(\"USE SCHEMA bronze;\")\n",
    "#ata_dl.write.format(\"delta\").mode(\"append\").save(\"hdfs://hdpmaster:9000/deltalake/bronze.tags\")\n",
    "data_dl.write.format(\"delta\").mode(\"append\").saveAsTable(name='bronze.tags')\n",
    "\n",
    "# mode = append, overwrite, error, errorifexists, ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05150b75",
   "metadata": {},
   "source": [
    "### Read data in table - Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e6ff795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+--------------------+-----------------+------------------+----------------+-----------------+\n",
      "|userId|movieId|                 tag| timestamp|    created_datetime|created_date_year|created_date_month|created_date_day|modified_datetime|\n",
      "+------+-------+--------------------+----------+--------------------+-----------------+------------------+----------------+-----------------+\n",
      "|     3|    260|             classic|1439472355|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     3|    260|              sci-fi|1439472256|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4| 168250|       unpredictable|1573945171|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4|   1732|         dark comedy|1573943598|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4|   1732|      great dialogue|1573943604|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4|   7569|    so bad it's good|1573943455|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4|  44665|unreliable narrators|1573943619|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4| 115569|               tense|1573943077|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4| 115713|artificial intell...|1573942979|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4| 115713|       philosophical|1573943033|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4| 115713|               tense|1573943042|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4| 148426|    so bad it's good|1573942965|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4| 164909|              cliche|1573943721|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4| 164909|             musical|1573943714|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "|     4| 168250|              horror|1573945163|2022-10-20 18:29:...|             2022|                10|              20|             null|\n",
      "+------+-------+--------------------+----------+--------------------+-----------------+------------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_session_delta.sql(\"SELECT * FROM bronze.tags ORDER BY userId LIMIT 15;\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c83d25c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1093360|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_session_delta.sql(\"SELECT COUNT(*) FROM bronze.tags;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4fd17",
   "metadata": {},
   "source": [
    "### EXPLAIN statement is used to provide logical/physical plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85f32fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(plan=\"== Parsed Logical Plan ==\\n'GlobalLimit 150\\n+- 'LocalLimit 150\\n   +- 'Sort ['userId ASC NULLS FIRST], true\\n      +- 'Project [*]\\n         +- 'UnresolvedRelation [bronze, tags], [], false\\n\\n== Analyzed Logical Plan ==\\nuserId: int, movieId: int, tag: string, timestamp: int, created_datetime: timestamp, created_date_year: int, created_date_month: int, created_date_day: int, modified_datetime: timestamp\\nGlobalLimit 150\\n+- LocalLimit 150\\n   +- Sort [userId#1533 ASC NULLS FIRST], true\\n      +- Project [userId#1533, movieId#1534, tag#1535, timestamp#1536, created_datetime#1537, created_date_year#1538, created_date_month#1539, created_date_day#1540, modified_datetime#1541]\\n         +- SubqueryAlias spark_catalog.bronze.tags\\n            +- Relation bronze.tags[userId#1533,movieId#1534,tag#1535,timestamp#1536,created_datetime#1537,created_date_year#1538,created_date_month#1539,created_date_day#1540,modified_datetime#1541] parquet\\n\\n== Optimized Logical Plan ==\\nGlobalLimit 150\\n+- LocalLimit 150\\n   +- Sort [userId#1533 ASC NULLS FIRST], true\\n      +- Relation bronze.tags[userId#1533,movieId#1534,tag#1535,timestamp#1536,created_datetime#1537,created_date_year#1538,created_date_month#1539,created_date_day#1540,modified_datetime#1541] parquet\\n\\n== Physical Plan ==\\nTakeOrderedAndProject(limit=150, orderBy=[userId#1533 ASC NULLS FIRST], output=[userId#1533,movieId#1534,tag#1535,timestamp#1536,created_datetime#1537,created_date_year#1538,created_date_month#1539,created_date_day#1540,modified_datetime#1541])\\n+- *(1) ColumnarToRow\\n   +- FileScan parquet bronze.tags[userId#1533,movieId#1534,tag#1535,timestamp#1536,created_datetime#1537,created_date_day#1540,modified_datetime#1541,created_date_year#1538,created_date_month#1539] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[hdfs://hdpmaster:9000/deltalake/bronze.tags], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<userId:int,movieId:int,tag:string,timestamp:int,created_datetime:timestamp,created_date_da...\\n\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"EXPLAIN EXTENDED SELECT * FROM bronze.tags ORDER BY userId LIMIT 150;\").head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bad6743f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(plan='Found 1 WholeStageCodegen subtrees.\\n== Subtree 1 / 1 (maxMethodCodeSize:562; maxConstantPoolSize:145(0.22% used); numInnerClasses:0) ==\\n*(1) ColumnarToRow\\n+- FileScan parquet bronze.tags[userId#1613,movieId#1614,tag#1615,timestamp#1616,created_datetime#1617,created_date_day#1620,modified_datetime#1621,created_date_year#1618,created_date_month#1619] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[hdfs://hdpmaster:9000/deltalake/bronze.tags], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<userId:int,movieId:int,tag:string,timestamp:int,created_datetime:timestamp,created_date_da...\\n\\nGenerated code:\\n/* 001 */ public Object generate(Object[] references) {\\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\\n/* 003 */ }\\n/* 004 */\\n/* 005 */ // codegenStageId=1\\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\\n/* 007 */   private Object[] references;\\n/* 008 */   private scala.collection.Iterator[] inputs;\\n/* 009 */   private int columnartorow_batchIdx_0;\\n/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[9];\\n/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\\n/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\\n/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\\n/* 014 */\\n/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\\n/* 016 */     this.references = references;\\n/* 017 */   }\\n/* 018 */\\n/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\\n/* 020 */     partitionIndex = index;\\n/* 021 */     this.inputs = inputs;\\n/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];\\n/* 023 */\\n/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 32);\\n/* 025 */\\n/* 026 */   }\\n/* 027 */\\n/* 028 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\\n/* 029 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\\n/* 030 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\\n/* 031 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\\n/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\\n/* 033 */       columnartorow_batchIdx_0 = 0;\\n/* 034 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);\\n/* 035 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);\\n/* 036 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);\\n/* 037 */       columnartorow_mutableStateArray_2[3] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(3);\\n/* 038 */       columnartorow_mutableStateArray_2[4] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(4);\\n/* 039 */       columnartorow_mutableStateArray_2[5] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(5);\\n/* 040 */       columnartorow_mutableStateArray_2[6] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(6);\\n/* 041 */       columnartorow_mutableStateArray_2[7] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(7);\\n/* 042 */       columnartorow_mutableStateArray_2[8] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(8);\\n/* 043 */\\n/* 044 */     }\\n/* 045 */   }\\n/* 046 */\\n/* 047 */   protected void processNext() throws java.io.IOException {\\n/* 048 */     if (columnartorow_mutableStateArray_1[0] == null) {\\n/* 049 */       columnartorow_nextBatch_0();\\n/* 050 */     }\\n/* 051 */     while ( columnartorow_mutableStateArray_1[0] != null) {\\n/* 052 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\\n/* 053 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\\n/* 054 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\\n/* 055 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\\n/* 056 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\\n/* 057 */         int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));\\n/* 058 */         boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\\n/* 059 */         int columnartorow_value_1 = columnartorow_isNull_1 ? -1 : (columnartorow_mutableStateArray_2[1].getInt(columnartorow_rowIdx_0));\\n/* 060 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\\n/* 061 */         UTF8String columnartorow_value_2 = columnartorow_isNull_2 ? null : (columnartorow_mutableStateArray_2[2].getUTF8String(columnartorow_rowIdx_0));\\n/* 062 */         boolean columnartorow_isNull_3 = columnartorow_mutableStateArray_2[3].isNullAt(columnartorow_rowIdx_0);\\n/* 063 */         int columnartorow_value_3 = columnartorow_isNull_3 ? -1 : (columnartorow_mutableStateArray_2[3].getInt(columnartorow_rowIdx_0));\\n/* 064 */         long columnartorow_value_4 = columnartorow_mutableStateArray_2[4].getLong(columnartorow_rowIdx_0);\\n/* 065 */         int columnartorow_value_5 = columnartorow_mutableStateArray_2[5].getInt(columnartorow_rowIdx_0);\\n/* 066 */         boolean columnartorow_isNull_4 = columnartorow_mutableStateArray_2[6].isNullAt(columnartorow_rowIdx_0);\\n/* 067 */         long columnartorow_value_6 = columnartorow_isNull_4 ? -1L : (columnartorow_mutableStateArray_2[6].getLong(columnartorow_rowIdx_0));\\n/* 068 */         int columnartorow_value_7 = columnartorow_mutableStateArray_2[7].getInt(columnartorow_rowIdx_0);\\n/* 069 */         int columnartorow_value_8 = columnartorow_mutableStateArray_2[8].getInt(columnartorow_rowIdx_0);\\n/* 070 */         columnartorow_mutableStateArray_3[0].reset();\\n/* 071 */\\n/* 072 */         columnartorow_mutableStateArray_3[0].zeroOutNullBytes();\\n/* 073 */\\n/* 074 */         if (columnartorow_isNull_0) {\\n/* 075 */           columnartorow_mutableStateArray_3[0].setNullAt(0);\\n/* 076 */         } else {\\n/* 077 */           columnartorow_mutableStateArray_3[0].write(0, columnartorow_value_0);\\n/* 078 */         }\\n/* 079 */\\n/* 080 */         if (columnartorow_isNull_1) {\\n/* 081 */           columnartorow_mutableStateArray_3[0].setNullAt(1);\\n/* 082 */         } else {\\n/* 083 */           columnartorow_mutableStateArray_3[0].write(1, columnartorow_value_1);\\n/* 084 */         }\\n/* 085 */\\n/* 086 */         if (columnartorow_isNull_2) {\\n/* 087 */           columnartorow_mutableStateArray_3[0].setNullAt(2);\\n/* 088 */         } else {\\n/* 089 */           columnartorow_mutableStateArray_3[0].write(2, columnartorow_value_2);\\n/* 090 */         }\\n/* 091 */\\n/* 092 */         if (columnartorow_isNull_3) {\\n/* 093 */           columnartorow_mutableStateArray_3[0].setNullAt(3);\\n/* 094 */         } else {\\n/* 095 */           columnartorow_mutableStateArray_3[0].write(3, columnartorow_value_3);\\n/* 096 */         }\\n/* 097 */\\n/* 098 */         columnartorow_mutableStateArray_3[0].write(4, columnartorow_value_4);\\n/* 099 */\\n/* 100 */         columnartorow_mutableStateArray_3[0].write(5, columnartorow_value_5);\\n/* 101 */\\n/* 102 */         if (columnartorow_isNull_4) {\\n/* 103 */           columnartorow_mutableStateArray_3[0].setNullAt(6);\\n/* 104 */         } else {\\n/* 105 */           columnartorow_mutableStateArray_3[0].write(6, columnartorow_value_6);\\n/* 106 */         }\\n/* 107 */\\n/* 108 */         columnartorow_mutableStateArray_3[0].write(7, columnartorow_value_7);\\n/* 109 */\\n/* 110 */         columnartorow_mutableStateArray_3[0].write(8, columnartorow_value_8);\\n/* 111 */         append((columnartorow_mutableStateArray_3[0].getRow()));\\n/* 112 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\\n/* 113 */       }\\n/* 114 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\\n/* 115 */       columnartorow_mutableStateArray_1[0] = null;\\n/* 116 */       columnartorow_nextBatch_0();\\n/* 117 */     }\\n/* 118 */   }\\n/* 119 */\\n/* 120 */ }\\n\\n')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"EXPLAIN CODEGEN SELECT * FROM bronze.tags ORDER BY userId LIMIT 150;\").head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce521f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(plan='== Optimized Logical Plan ==\\nGlobalLimit 150, Statistics(sizeInBytes=10.0 KiB, rowCount=150)\\n+- LocalLimit 150, Statistics(sizeInBytes=21.0 MiB)\\n   +- Sort [userId#1693 ASC NULLS FIRST], true, Statistics(sizeInBytes=21.0 MiB)\\n      +- Relation bronze.tags[userId#1693,movieId#1694,tag#1695,timestamp#1696,created_datetime#1697,created_date_year#1698,created_date_month#1699,created_date_day#1700,modified_datetime#1701] parquet, Statistics(sizeInBytes=21.0 MiB)\\n\\n== Physical Plan ==\\nTakeOrderedAndProject(limit=150, orderBy=[userId#1693 ASC NULLS FIRST], output=[userId#1693,movieId#1694,tag#1695,timestamp#1696,created_datetime#1697,created_date_year#1698,created_date_month#1699,created_date_day#1700,modified_datetime#1701])\\n+- *(1) ColumnarToRow\\n   +- FileScan parquet bronze.tags[userId#1693,movieId#1694,tag#1695,timestamp#1696,created_datetime#1697,created_date_day#1700,modified_datetime#1701,created_date_year#1698,created_date_month#1699] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[hdfs://hdpmaster:9000/deltalake/bronze.tags], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<userId:int,movieId:int,tag:string,timestamp:int,created_datetime:timestamp,created_date_da...\\n\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"EXPLAIN COST SELECT * FROM bronze.tags ORDER BY userId LIMIT 150;\").head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "148e53ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(plan='== Physical Plan ==\\nTakeOrderedAndProject (3)\\n+- * ColumnarToRow (2)\\n   +- Scan parquet bronze.tags (1)\\n\\n\\n(1) Scan parquet bronze.tags\\nOutput [9]: [userId#1773, movieId#1774, tag#1775, timestamp#1776, created_datetime#1777, created_date_day#1780, modified_datetime#1781, created_date_year#1778, created_date_month#1779]\\nBatched: true\\nLocation: PreparedDeltaFileIndex [hdfs://hdpmaster:9000/deltalake/bronze.tags]\\nReadSchema: struct<userId:int,movieId:int,tag:string,timestamp:int,created_datetime:timestamp,created_date_day:int,modified_datetime:timestamp>\\n\\n(2) ColumnarToRow [codegen id : 1]\\nInput [9]: [userId#1773, movieId#1774, tag#1775, timestamp#1776, created_datetime#1777, created_date_day#1780, modified_datetime#1781, created_date_year#1778, created_date_month#1779]\\n\\n(3) TakeOrderedAndProject\\nInput [9]: [userId#1773, movieId#1774, tag#1775, timestamp#1776, created_datetime#1777, created_date_day#1780, modified_datetime#1781, created_date_year#1778, created_date_month#1779]\\nArguments: 150, [userId#1773 ASC NULLS FIRST], [userId#1773, movieId#1774, tag#1775, timestamp#1776, created_datetime#1777, created_date_year#1778, created_date_month#1779, created_date_day#1780, modified_datetime#1781]\\n\\n')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"EXPLAIN FORMATTED SELECT * FROM bronze.tags ORDER BY userId LIMIT 150;\").head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e9dd9",
   "metadata": {},
   "source": [
    "### Get information about Delta objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "364aa4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(format='delta', id='772a8dd7-e548-4ec4-ad09-f1ac0b88b1c3', name=None, description='Table to store the genre for each movie.', location='hdfs://hdpmaster:9000/deltalake/bronze/tags', createdAt=datetime.datetime(2022, 10, 20, 18, 28, 18, 228000), lastModified=datetime.datetime(2022, 10, 20, 18, 29, 6, 225000), partitionColumns=['created_date_year', 'created_date_month'], numFiles=48, sizeInBytes=11238012, properties={'description': 'Table to store the genre for each movie.'}, minReaderVersion=1, minWriterVersion=4)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable_tags_by_path = DeltaTable.forPath(sparkSession=s_session_delta, path=\"hdfs://hdpmaster:9000/deltalake/bronze/tags\")\n",
    "deltaTable_tags_by_path.detail().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "500841df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(format='delta', id='772a8dd7-e548-4ec4-ad09-f1ac0b88b1c3', name=None, description='Table to store the genre for each movie.', location='hdfs://hdpmaster:9000/deltalake/bronze/tags', createdAt=datetime.datetime(2022, 10, 20, 18, 28, 18, 228000), lastModified=datetime.datetime(2022, 10, 20, 18, 29, 6, 225000), partitionColumns=['created_date_year', 'created_date_month'], numFiles=48, sizeInBytes=11238012, properties={'description': 'Table to store the genre for each movie.'}, minReaderVersion=1, minWriterVersion=4)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"DESCRIBE DETAIL 'hdfs://hdpmaster:9000/deltalake/bronze/tags'\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ba2ccd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(version=1, timestamp=datetime.datetime(2022, 10, 20, 18, 29, 6, 225000), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'Append', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=0, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={'numOutputRows': '1093360', 'numOutputBytes': '11238012', 'numFiles': '48'}, userMetadata=None, engineInfo='Apache-Spark/3.3.0 Delta-Lake/2.1.0'),\n",
       " Row(version=0, timestamp=datetime.datetime(2022, 10, 20, 18, 28, 18, 275000), userId=None, userName=None, operation='CREATE TABLE', operationParameters={'description': 'Table to store the genre for each movie.', 'partitionBy': '[\"created_date_year\",\"created_date_month\"]', 'properties': '{\"description\":\"Table to store the genre for each movie.\"}', 'isManaged': 'false'}, job=None, notebook=None, clusterId=None, readVersion=None, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={}, userMetadata=None, engineInfo='Apache-Spark/3.3.0 Delta-Lake/2.1.0')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable_tags_by_path.history().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99f6a676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(version=1, timestamp=datetime.datetime(2022, 10, 20, 18, 29, 6, 225000), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'Append', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=0, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={'numOutputRows': '1093360', 'numOutputBytes': '11238012', 'numFiles': '48'}, userMetadata=None, engineInfo='Apache-Spark/3.3.0 Delta-Lake/2.1.0'),\n",
       " Row(version=0, timestamp=datetime.datetime(2022, 10, 20, 18, 28, 18, 275000), userId=None, userName=None, operation='CREATE TABLE', operationParameters={'description': 'Table to store the genre for each movie.', 'partitionBy': '[\"created_date_year\",\"created_date_month\"]', 'properties': '{\"description\":\"Table to store the genre for each movie.\"}', 'isManaged': 'false'}, job=None, notebook=None, clusterId=None, readVersion=None, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={}, userMetadata=None, engineInfo='Apache-Spark/3.3.0 Delta-Lake/2.1.0')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"DESCRIBE HISTORY 'hdfs://hdpmaster:9000/deltalake/bronze/tags' LIMIT 10\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "50fdc113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(format='delta', id='772a8dd7-e548-4ec4-ad09-f1ac0b88b1c3', name=None, description='Table to store the genre for each movie.', location='hdfs://hdpmaster:9000/deltalake/bronze/tags', createdAt=datetime.datetime(2022, 10, 20, 18, 28, 18, 228000), lastModified=datetime.datetime(2022, 10, 20, 18, 29, 6, 225000), partitionColumns=['created_date_year', 'created_date_month'], numFiles=48, sizeInBytes=11238012, properties={'description': 'Table to store the genre for each movie.'}, minReaderVersion=1, minWriterVersion=4)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable_tags_by_tablename = DeltaTable.forName(sparkSession=s_session_delta,tableOrViewName='bronze.tags')\n",
    "deltaTable_tags_by_tablename.detail().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7993ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(format='delta', id='772a8dd7-e548-4ec4-ad09-f1ac0b88b1c3', name='bronze.tags', description='Table to store the genre for each movie.', location='hdfs://hdpmaster:9000/deltalake/bronze/tags', createdAt=datetime.datetime(2022, 10, 20, 18, 28, 18, 228000), lastModified=datetime.datetime(2022, 10, 20, 18, 29, 6, 225000), partitionColumns=['created_date_year', 'created_date_month'], numFiles=48, sizeInBytes=11238012, properties={'description': 'Table to store the genre for each movie.'}, minReaderVersion=1, minWriterVersion=4)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"DESCRIBE DETAIL bronze.tags\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c79f638a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(version=2, timestamp=datetime.datetime(2022, 10, 20, 17, 40, 53, 170000), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'Append', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=1, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={'numOutputRows': '1093360', 'numOutputBytes': '11022615', 'numFiles': '36'}, userMetadata=None, engineInfo='Apache-Spark/3.3.0 Delta-Lake/2.1.0'),\n",
       " Row(version=1, timestamp=datetime.datetime(2022, 10, 20, 16, 20, 9, 834000), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'Append', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=0, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={'numOutputRows': '1093360', 'numOutputBytes': '11022615', 'numFiles': '36'}, userMetadata=None, engineInfo='Apache-Spark/3.3.0 Delta-Lake/2.1.0'),\n",
       " Row(version=0, timestamp=datetime.datetime(2022, 10, 20, 16, 11, 14, 399000), userId=None, userName=None, operation='CREATE TABLE', operationParameters={'description': 'Table to store the genre for each movie.', 'partitionBy': '[\"created_date_year\",\"created_date_month\"]', 'properties': '{\"description\":\"Table to store the genre for each movie.\"}', 'isManaged': 'false'}, job=None, notebook=None, clusterId=None, readVersion=None, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={}, userMetadata=None, engineInfo='Apache-Spark/3.3.0 Delta-Lake/2.1.0')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable_tags_by_tablename.history().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bac9d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(version=1, timestamp=datetime.datetime(2022, 10, 20, 18, 29, 6, 225000), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'Append', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=0, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={'numOutputRows': '1093360', 'numOutputBytes': '11238012', 'numFiles': '48'}, userMetadata=None, engineInfo='Apache-Spark/3.3.0 Delta-Lake/2.1.0'),\n",
       " Row(version=0, timestamp=datetime.datetime(2022, 10, 20, 18, 28, 18, 275000), userId=None, userName=None, operation='CREATE TABLE', operationParameters={'description': 'Table to store the genre for each movie.', 'partitionBy': '[\"created_date_year\",\"created_date_month\"]', 'properties': '{\"description\":\"Table to store the genre for each movie.\"}', 'isManaged': 'false'}, job=None, notebook=None, clusterId=None, readVersion=None, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={}, userMetadata=None, engineInfo='Apache-Spark/3.3.0 Delta-Lake/2.1.0')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"DESCRIBE HISTORY bronze.tags\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4f29791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_name='userId'),\n",
       " Row(col_name='movieId'),\n",
       " Row(col_name='tag'),\n",
       " Row(col_name='timestamp'),\n",
       " Row(col_name='created_datetime'),\n",
       " Row(col_name='created_date_year'),\n",
       " Row(col_name='created_date_month'),\n",
       " Row(col_name='created_date_day'),\n",
       " Row(col_name='modified_datetime')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"SHOW COLUMNS IN  bronze.tags\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b934672e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_name='userId', data_type='int', comment=''),\n",
       " Row(col_name='movieId', data_type='int', comment=''),\n",
       " Row(col_name='tag', data_type='string', comment='Movie genre.'),\n",
       " Row(col_name='timestamp', data_type='int', comment=''),\n",
       " Row(col_name='created_datetime', data_type='timestamp', comment=''),\n",
       " Row(col_name='created_date_year', data_type='int', comment=''),\n",
       " Row(col_name='created_date_month', data_type='int', comment=''),\n",
       " Row(col_name='created_date_day', data_type='int', comment=''),\n",
       " Row(col_name='modified_datetime', data_type='timestamp', comment=''),\n",
       " Row(col_name='', data_type='', comment='')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"DESCRIBE TABLE bronze.tags\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "767eb438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_name='userId', data_type='int', comment=''),\n",
       " Row(col_name='movieId', data_type='int', comment=''),\n",
       " Row(col_name='tag', data_type='string', comment='Movie genre.'),\n",
       " Row(col_name='timestamp', data_type='int', comment=''),\n",
       " Row(col_name='created_datetime', data_type='timestamp', comment=''),\n",
       " Row(col_name='created_date_year', data_type='int', comment=''),\n",
       " Row(col_name='created_date_month', data_type='int', comment=''),\n",
       " Row(col_name='created_date_day', data_type='int', comment=''),\n",
       " Row(col_name='modified_datetime', data_type='timestamp', comment=''),\n",
       " Row(col_name='', data_type='', comment='')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session_delta.sql(\"DESCRIBE TABLE EXTENDED bronze.tags\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb5c7e",
   "metadata": {},
   "source": [
    "### Optimize table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b4206453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable_tags_by_tablename.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a17697ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s_session_delta.sql(\"DROP TABLE IF EXISTS bronze.tags\").head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
