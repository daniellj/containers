{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40353901",
   "metadata": {},
   "source": [
    "### O.S. process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b29426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hostname: jupyter-hub\n",
      "- JAVA_HOME:/opt/jdk\n",
      "- PYSPARK_SUBMIT_ARGS:--packages io.delta:delta-core_2.12:2.1.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" pyspark-shell\n",
      "- PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/jdk:/opt/jdk/bin\n",
      "dbus-python==1.2.18\n",
      "delta-spark==2.1.0\n",
      "gyp==0.1\n",
      "importlib-metadata==5.0.0\n",
      "numpy==1.23.4\n",
      "py4j==0.10.9.5\n",
      "pyarrow==9.0.0\n",
      "PyGObject==3.42.1\n",
      "pyspark==3.3.0\n",
      "zipp==3.9.0\n",
      "java version \"1.8.0_341\"\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_341-b10)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.341-b10, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from os import environ\n",
    "\n",
    "!host=$(hostname) \n",
    "!ip=$(ifconfig | grep 'inet ' | grep -v '127.0.0.1' | cut -c 7-17)\n",
    "!echo \"hostname: $(hostname)\"\n",
    "\n",
    "# create path if not exists\n",
    "#!mkdir -p ~/notebooks/data/\n",
    "\n",
    "# download if not exists\n",
    "#!wget -nc https://files.grouplens.org/datasets/movielens/ml-25m.zip -P ~/notebooks/data/\n",
    "\n",
    "# unzip if not exists\n",
    "#!unzip -n ~/notebooks/data/ml-25m.zip -d ~/notebooks/data/\n",
    "\n",
    "#!ls -las /home/admin/notebooks/data/ml-25m\n",
    "\n",
    "# check environment variables: JAVA_HOME\n",
    "!export JAVA_HOME=/opt/jdk\n",
    "environ[\"JAVA_HOME\"] = \"/opt/jdk\"\n",
    "!echo \"- JAVA_HOME:$JAVA_HOME\"\n",
    "\n",
    "# check environment variables: PYSPARK_SUBMIT_ARGS\n",
    "!export PYSPARK_SUBMIT_ARGS='--packages io.delta:delta-core_2.12:2.1.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" pyspark-shell'\n",
    "environ[\"PYSPARK_SUBMIT_ARGS\"]='--packages io.delta:delta-core_2.12:2.1.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" pyspark-shell'\n",
    "!echo \"- PYSPARK_SUBMIT_ARGS:$PYSPARK_SUBMIT_ARGS\"\n",
    "\n",
    "# check environment variables: PATH\n",
    "!export PATH=$PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/jdk:/opt/jdk/bin\n",
    "environ[\"PATH\"] = \"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/jdk:/opt/jdk/bin\"\n",
    "!echo \"- PATH:$PATH\"\n",
    "\n",
    "!pip freeze\n",
    "\n",
    "# check java version\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b77c33",
   "metadata": {},
   "source": [
    "### Function to reduce memory usage in Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1825f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%\\n'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d353c",
   "metadata": {},
   "source": [
    "### Connect from Apache Spark Cluster - without Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2bb9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/srv/jupyterhub/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/admin/.ivy2/cache\n",
      "The jars for the packages stored in: /home/admin/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-90bea128-ec4f-4635-bbc3-dedb762cec9c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 134ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-90bea128-ec4f-4635-bbc3-dedb762cec9c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/25 13:36:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#from os import environ\n",
    "#environ[\"SPARK_HOME\"] = '/opt/apache-spark'\n",
    "#environ[\"PATH\"] = '$PATH:/opt/jdk:/opt/jdk/bin:/opt/apache-spark:/opt/apache-spark/bin:/opt/apache-spark/sbin'\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrameReader\n",
    "\n",
    "#If you need to stop SparkContext (sc) or SparkSession\n",
    "if 's_session' in locals():\n",
    "    s_session.stop()\n",
    "if 's_context' in locals():\n",
    "    s_context.stop()\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.setAppName(\"app_data_lake\") \\\n",
    ".setMaster(\"spark://spark-master:7077\")\n",
    "#.setSparkHome(\"/opt/apache-spark\")\n",
    "\n",
    "s_context = SparkContext(conf=conf).getOrCreate()\n",
    "s_session = SparkSession(sparkContext=s_context)\n",
    "#builder = s_session.builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb7ed0",
   "metadata": {},
   "source": [
    "### Read .CSV from SFTP and load into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdaec2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection succesfully establishedâ€¦\n",
      "\n",
      "Chunksize block = line 0 until 500000\n",
      "Memory usage of dataframe is 15.26 MB\n",
      "Memory usage after optimization is: 8.99 MB\n",
      "Decreased by 41.1%\n",
      "\n",
      "Chunksize block = line 500001 until 1000000\n",
      "Memory usage of dataframe is 15.26 MB\n",
      "Memory usage after optimization is: 8.97 MB\n",
      "Decreased by 41.2%\n",
      "\n",
      "Chunksize block = line 1000001 until 1500000\n",
      "Memory usage of dataframe is 2.85 MB\n",
      "Memory usage after optimization is: 1.88 MB\n",
      "Decreased by 33.9%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pysftp\n",
    "from pandas import read_csv as pandas_read_csv\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# SFTP config connection\n",
    "cnopts = pysftp.CnOpts()\n",
    "cnopts.hostkeys = None\n",
    "environ[\"FTP_HOST\"] = 'sftp-01' # sftp-01 = 172.19.0.15\n",
    "environ[\"FTP_PORT\"] = '2222'\n",
    "environ[\"FTP_USER\"] = 'admin'\n",
    "environ[\"FTP_PASS\"] = 'admin'\n",
    "\n",
    "# CSV schema\n",
    "schema_doc = {\n",
    "                \"tags.csv\": StructType([StructField(\"userId\", IntegerType(), True),\n",
    "                                     StructField(\"movieId\", IntegerType(), True),\n",
    "                                     StructField(\"tag\", StringType(), True),\n",
    "                                     StructField(\"timestamp\", IntegerType(), True)]),\n",
    "                \"ratings.csv\": StructType([StructField(\"userId\", IntegerType(), True),\n",
    "                                     StructField(\"movieId\", IntegerType(), True),\n",
    "                                     StructField(\"rating\", FloatType(), True),\n",
    "                                     StructField(\"timestamp\", IntegerType(), True)])\n",
    "                }\n",
    "\n",
    "chunksize=500000\n",
    "sftp_file=\"tags.csv\" # 1.093.000 lines\n",
    "#sftp_file=\"ratings.csv\" # 25.000.000 lines\n",
    "\n",
    "# open SFTP connection\n",
    "with pysftp.Connection(environ[\"FTP_HOST\"], port = int(environ[\"FTP_PORT\"]), username = environ[\"FTP_USER\"], password = environ[\"FTP_PASS\"], cnopts=cnopts) as connection:\n",
    "    print(\"Connection succesfully establishedâ€¦\\n\")\n",
    "    # open the file\n",
    "    with connection.open(remote_file = f\"/data/{sftp_file}\", mode='r') as file:\n",
    "\n",
    "        i = 1\n",
    "        for reader in pandas_read_csv(file, sep=',', chunksize=chunksize):\n",
    "            chnk = \"0 until \" + str(chunksize) if i==1 else str(((chunksize * i) - chunksize) + 1) + \" until \" + str(chunksize * i)\n",
    "            print(f\"Chunksize block = line {chnk}\")\n",
    "            reader=reduce_mem_usage(df=reader)\n",
    "            if i == 1:\n",
    "                data = s_session.createDataFrame(data=reader, schema=schema_doc.get(sftp_file, sftp_file.split('.')[0]))\n",
    "            else:\n",
    "                new_data = s_session.createDataFrame(data=reader, schema=schema_doc.get(sftp_file, sftp_file.split('.')[0]))\n",
    "                data = data.union(new_data)\n",
    "                del new_data\n",
    "            i = i + 1\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a7d82",
   "metadata": {},
   "source": [
    "### Resume of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a68de79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- sparkSession:  <pyspark.sql.session.SparkSession object at 0x7f593c54beb0> \n",
      "\n",
      "- Object:  <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "- schema:  StructType([StructField('userId', IntegerType(), True), StructField('movieId', IntegerType(), True), StructField('tag', StringType(), True), StructField('timestamp', IntegerType(), True)]) \n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "- printSchema:  None \n",
      "\n",
      "- isStreaming:  False \n",
      "\n",
      "- columns:  ['userId', 'movieId', 'tag', 'timestamp'] \n",
      "\n",
      "- dtypes:  [('userId', 'int'), ('movieId', 'int'), ('tag', 'string'), ('timestamp', 'int')] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- head:  [Row(userId=3, movieId=260, tag='classic', timestamp=1439472355), Row(userId=3, movieId=260, tag='sci-fi', timestamp=1439472256), Row(userId=4, movieId=1732, tag='dark comedy', timestamp=1573943598), Row(userId=4, movieId=1732, tag='great dialogue', timestamp=1573943604), Row(userId=4, movieId=7569, tag=\"so bad it's good\", timestamp=1573943455), Row(userId=4, movieId=44665, tag='unreliable narrators', timestamp=1573943619), Row(userId=4, movieId=115569, tag='tense', timestamp=1573943077), Row(userId=4, movieId=115713, tag='artificial intelligence', timestamp=1573942979), Row(userId=4, movieId=115713, tag='philosophical', timestamp=1573943033), Row(userId=4, movieId=115713, tag='tense', timestamp=1573943042)] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+\n",
      "|userId|movieId|                 tag| timestamp|\n",
      "+------+-------+--------------------+----------+\n",
      "|     3|    260|             classic|1439472355|\n",
      "|     3|    260|              sci-fi|1439472256|\n",
      "|     4|   1732|         dark comedy|1573943598|\n",
      "|     4|   1732|      great dialogue|1573943604|\n",
      "|     4|   7569|    so bad it's good|1573943455|\n",
      "|     4|  44665|unreliable narrators|1573943619|\n",
      "|     4| 115569|               tense|1573943077|\n",
      "|     4| 115713|artificial intell...|1573942979|\n",
      "|     4| 115713|       philosophical|1573943033|\n",
      "|     4| 115713|               tense|1573943042|\n",
      "+------+-------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "- show:  None \n",
      "\n",
      "- isEmpty:  False \n",
      "\n",
      "- cache DataFrame[userId: int, movieId: int, tag: string, timestamp: int] \n",
      "\n",
      "22/10/25 13:37:33 WARN CacheManager: Asked to cache already cached data.\n",
      "- persist:  DataFrame[userId: int, movieId: int, tag: string, timestamp: int] \n",
      "\n",
      "- storageLevel:  Disk Memory Deserialized 1x Replicated \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- count:  1093360 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+--------------------+\n",
      "|summary|           userId|           movieId|                 tag|           timestamp|\n",
      "+-------+-----------------+------------------+--------------------+--------------------+\n",
      "|  count|          1093360|           1093360|             1093360|             1093360|\n",
      "|   mean|67590.22463324065|  58492.7644389771|                 NaN|  1.43011549764337E9|\n",
      "| stddev|51521.13756056978|59687.312817478196|                 NaN|1.1773844833352971E8|\n",
      "|    min|                3|                 1| Alexander SkarsgÃ¥rd|          1135429210|\n",
      "|    25%|            15204|              3504|                 3.0|          1339252662|\n",
      "|    50%|            62199|             45928|                 3.5|          1468921818|\n",
      "|    75%|           113652|            102903|              1929.0|          1527402191|\n",
      "|    max|           162534|            209063|          ì¹´ìš´íŠ¸ë‹¤ìš´|          1574316696|\n",
      "+-------+-----------------+------------------+--------------------+--------------------+\n",
      "\n",
      "- summary:  None \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print( \"- sparkSession: \", data.sparkSession, '\\n' )\n",
    "print(\"- Object: \", type(data), \"\\n\")\n",
    "print( \"- schema: \", data.schema, '\\n' )\n",
    "print( \"- printSchema: \", data.printSchema(), '\\n' )\n",
    "print( \"- isStreaming: \", data.isStreaming, '\\n' )\n",
    "print( \"- columns: \", data.columns, '\\n' )\n",
    "print( \"- dtypes: \", data.dtypes, '\\n' )\n",
    "print( \"- head: \", data.head(10), '\\n' )\n",
    "print( \"- show: \", data.show(10), '\\n' )\n",
    "print( \"- isEmpty: \", data.isEmpty(), '\\n' )\n",
    "print(\"- cache\", data.cache(), '\\n' ) # Persists the DataFrame with the default storage level (MEMORY_AND_DISK)\n",
    "print( \"- persist: \", data.persist(), '\\n' ) # Sets the storage level to persist the contents of the DataFrame across operations after the first time it is computed.\n",
    "print( \"- storageLevel: \", data.storageLevel, '\\n' )\n",
    "print( \"- count: \", data.count(), '\\n' )\n",
    "if sftp_file==\"ratings.csv\":\n",
    "    print( \"- correlation between rating and timestamp: \", data.corr(\"rating\", \"timestamp\"), '\\n' )\n",
    "    print( \"- covariance between rating and timestamp: \", data.cov(\"rating\", \"timestamp\"), '\\n' ) # Calculate the sample covariance for the given columns, specified by their names, as a double value\n",
    "    print( \"- descriptive statistics: \", data.describe([\"userId\", \"movieId\", \"rating\", \"timestamp\"]).show(), '\\n' )\n",
    "print( \"- summary: \", data.summary().show(), '\\n' ) # Computes specified statistics for numeric and string columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ec9ea",
   "metadata": {},
   "source": [
    "### Write DataFrame in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ecb27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.csv(\"hdfs://hdpmaster:9000/users/hduser/teste1.csv\", header=True, mode=\"ignore\")\n",
    "data.write.parquet(\"hdfs://hdpmaster:9000/users/hduser/teste1.parquet\", mode=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16c46b",
   "metadata": {},
   "source": [
    "### Read data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "040ec5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_load_csv = s_session.read.csv(\"hdfs://hdpmaster:9000/users/hduser/teste1.csv\", header='true', inferSchema='true')\n",
    "df_load_parquet = s_session.read.parquet(\"hdfs://hdpmaster:9000/users/hduser/teste1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd78427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV FILE: \n",
      "\n",
      "- sparkSession:  <pyspark.sql.session.SparkSession object at 0x7f593c54beb0> \n",
      "\n",
      "- Object:  <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "- schema:  StructType([StructField('userId', IntegerType(), True), StructField('movieId', IntegerType(), True), StructField('tag', StringType(), True), StructField('timestamp', IntegerType(), True)]) \n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "- printSchema:  None \n",
      "\n",
      "- isStreaming:  False \n",
      "\n",
      "- columns:  ['userId', 'movieId', 'tag', 'timestamp'] \n",
      "\n",
      "- dtypes:  [('userId', 'int'), ('movieId', 'int'), ('tag', 'string'), ('timestamp', 'int')] \n",
      "\n",
      "- head:  [Row(userId=61624, movieId=193477, tag='production design', timestamp=1541821771), Row(userId=61624, movieId=193477, tag='scotland', timestamp=1541821809), Row(userId=61624, movieId=193477, tag='sword and sandals', timestamp=1541821819), Row(userId=61624, movieId=193477, tag='tone', timestamp=1541872454), Row(userId=61624, movieId=200814, tag='Kiernan Shipka', timestamp=1554957611), Row(userId=61624, movieId=200814, tag='script', timestamp=1554957625), Row(userId=61624, movieId=200814, tag='Stanley Tucci', timestamp=1554957622), Row(userId=61626, movieId=724, tag='magic', timestamp=1375489450), Row(userId=61626, movieId=2810, tag='Satoshi Kon', timestamp=1375246095), Row(userId=61626, movieId=4370, tag='dystopia', timestamp=1375246046)] \n",
      "\n",
      "+------+-------+-----------------+----------+\n",
      "|userId|movieId|              tag| timestamp|\n",
      "+------+-------+-----------------+----------+\n",
      "| 61624| 193477|production design|1541821771|\n",
      "| 61624| 193477|         scotland|1541821809|\n",
      "| 61624| 193477|sword and sandals|1541821819|\n",
      "| 61624| 193477|             tone|1541872454|\n",
      "| 61624| 200814|   Kiernan Shipka|1554957611|\n",
      "| 61624| 200814|           script|1554957625|\n",
      "| 61624| 200814|    Stanley Tucci|1554957622|\n",
      "| 61626|    724|            magic|1375489450|\n",
      "| 61626|   2810|      Satoshi Kon|1375246095|\n",
      "| 61626|   4370|         dystopia|1375246046|\n",
      "+------+-------+-----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "- show:  None \n",
      "\n",
      "##########################################################\n",
      "PARQUET FILE: \n",
      "\n",
      "- sparkSession:  <pyspark.sql.session.SparkSession object at 0x7f593c54beb0> \n",
      "\n",
      "- Object:  <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "- schema:  StructType([StructField('userId', IntegerType(), True), StructField('movieId', IntegerType(), True), StructField('tag', StringType(), True), StructField('timestamp', IntegerType(), True)]) \n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "- printSchema:  None \n",
      "\n",
      "- isStreaming:  False \n",
      "\n",
      "- columns:  ['userId', 'movieId', 'tag', 'timestamp'] \n",
      "\n",
      "- dtypes:  [('userId', 'int'), ('movieId', 'int'), ('tag', 'string'), ('timestamp', 'int')] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- head:  [Row(userId=61624, movieId=193477, tag='production design', timestamp=1541821771), Row(userId=61624, movieId=193477, tag='scotland', timestamp=1541821809), Row(userId=61624, movieId=193477, tag='sword and sandals', timestamp=1541821819), Row(userId=61624, movieId=193477, tag='tone', timestamp=1541872454), Row(userId=61624, movieId=200814, tag='Kiernan Shipka', timestamp=1554957611), Row(userId=61624, movieId=200814, tag='script', timestamp=1554957625), Row(userId=61624, movieId=200814, tag='Stanley Tucci', timestamp=1554957622), Row(userId=61626, movieId=724, tag='magic', timestamp=1375489450), Row(userId=61626, movieId=2810, tag='Satoshi Kon', timestamp=1375246095), Row(userId=61626, movieId=4370, tag='dystopia', timestamp=1375246046)] \n",
      "\n",
      "+------+-------+-----------------+----------+\n",
      "|userId|movieId|              tag| timestamp|\n",
      "+------+-------+-----------------+----------+\n",
      "| 61624| 193477|production design|1541821771|\n",
      "| 61624| 193477|         scotland|1541821809|\n",
      "| 61624| 193477|sword and sandals|1541821819|\n",
      "| 61624| 193477|             tone|1541872454|\n",
      "| 61624| 200814|   Kiernan Shipka|1554957611|\n",
      "| 61624| 200814|           script|1554957625|\n",
      "| 61624| 200814|    Stanley Tucci|1554957622|\n",
      "| 61626|    724|            magic|1375489450|\n",
      "| 61626|   2810|      Satoshi Kon|1375246095|\n",
      "| 61626|   4370|         dystopia|1375246046|\n",
      "+------+-------+-----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "- show:  None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CSV FILE:\", \"\\n\")\n",
    "print( \"- sparkSession: \", df_load_csv.sparkSession, '\\n' )\n",
    "print(\"- Object: \", type(df_load_csv), \"\\n\")\n",
    "print( \"- schema: \", df_load_csv.schema, '\\n' )\n",
    "print( \"- printSchema: \", df_load_csv.printSchema(), '\\n' )\n",
    "print( \"- isStreaming: \", df_load_csv.isStreaming, '\\n' )\n",
    "print( \"- columns: \", df_load_csv.columns, '\\n' )\n",
    "print( \"- dtypes: \", df_load_csv.dtypes, '\\n' )\n",
    "print( \"- head: \", df_load_csv.head(10), '\\n' )\n",
    "print( \"- show: \", df_load_csv.show(10), '\\n' )\n",
    "print(\"##########################################################\")\n",
    "print(\"PARQUET FILE:\", \"\\n\")\n",
    "print( \"- sparkSession: \", df_load_parquet.sparkSession, '\\n' )\n",
    "print(\"- Object: \", type(df_load_parquet), \"\\n\")\n",
    "print( \"- schema: \", df_load_parquet.schema, '\\n' )\n",
    "print( \"- printSchema: \", df_load_parquet.printSchema(), '\\n' )\n",
    "print( \"- isStreaming: \", df_load_parquet.isStreaming, '\\n' )\n",
    "print( \"- columns: \", df_load_parquet.columns, '\\n' )\n",
    "print( \"- dtypes: \", df_load_parquet.dtypes, '\\n' )\n",
    "print( \"- head: \", df_load_parquet.head(10), '\\n' )\n",
    "print( \"- show: \", df_load_parquet.show(10), '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a775b5",
   "metadata": {},
   "source": [
    "### Create a temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f106d700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_session.sql(\"CREATE TEMPORARY VIEW teste USING parquet OPTIONS (path \\\"hdfs://hdpmaster:9000/users/hduser/teste1.parquet\\\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bde8374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:============================>                            (6 + 6) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------------+----------+\n",
      "|userId|movieId|tag              |timestamp |\n",
      "+------+-------+-----------------+----------+\n",
      "|61624 |193477 |production design|1541821771|\n",
      "|61624 |193477 |scotland         |1541821809|\n",
      "|61624 |193477 |sword and sandals|1541821819|\n",
      "|61624 |193477 |tone             |1541872454|\n",
      "|61624 |200814 |Kiernan Shipka   |1554957611|\n",
      "|61624 |200814 |script           |1554957625|\n",
      "|61624 |200814 |Stanley Tucci    |1554957622|\n",
      "|61626 |724    |magic            |1375489450|\n",
      "|61626 |2810   |Satoshi Kon      |1375246095|\n",
      "|61626 |4370   |dystopia         |1375246046|\n",
      "+------+-------+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s_session.sql(\"select * from teste limit 10\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
